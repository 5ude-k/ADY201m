name: Crawl 2 Notebooks & Update SQL Daily

on:
  workflow_dispatch:          # Cho phép chạy thủ công
  schedule:
    - cron: "0 0 * * *"       # Chạy tự động mỗi ngày 00:00 UTC (07:00 VN)

jobs:
  update-sql:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          pip install pandas jupyter nbconvert beautifulsoup4 requests openpyxl

      # --- 1️⃣ Chạy notebook BeautifulSoup ---
      - name: Run BeautifulSoup crawler
        run: |
          if [ ! -f "beautifulsoupcaocao.ipynb" ]; then
            echo "❌ Không tìm thấy file beautifulsoupcaocao.ipynb"; exit 1;
          fi
          jupyter nbconvert --to python beautifulsoupcaocao.ipynb --output beautifulsoupcaocao.py
          python beautifulsoupcaocao.py > crawl_output_1.txt

      # --- 2️⃣ Chạy notebook APICAO ---
      - name: Run APICAO crawler
        run: |
          if [ ! -f "APICAO.ipynb" ]; then
            echo "❌ Không tìm thấy file APICAO.ipynb"; exit 1;
          fi
          jupyter nbconvert --to python APICAO.ipynb --output APICAO.py
          python APICAO.py > crawl_output_2.txt

      # --- 3️⃣ Gộp CSV và cập nhật SQL ---
      - name: Merge CSV outputs and update SQL
        run: |
          python - <<'EOF'
          import pandas as pd, os
          from io import StringIO

          sql_file = "phongtro_danang.sql"

          def extract_csv(path):
              if not os.path.exists(path):
                  print(f"⚠️ File {path} không tồn tại")
                  return pd.DataFrame()
              with open(path, "r", encoding="utf-8") as f:
                  lines = f.read().splitlines()
              for i, line in enumerate(lines):
                  if line.lower().startswith("location,"):
                      return pd.read_csv(StringIO("\n".join(lines[i:])))
              return pd.DataFrame()

          # --- Gộp dữ liệu từ 2 file ---
          df1 = extract_csv("crawl_output_1.txt")
          df2 = extract_csv("crawl_output_2.txt")
          df = pd.concat([df1, df2], ignore_index=True).drop_duplicates()

          if df.empty:
              print("⚠️ Không có dữ liệu để cập nhật.")
              exit(0)

          df["location"] = df["location"].astype(str).str.replace("'", "", regex=False)

          # --- Đọc SQL cũ ---
          existing = set()
          if os.path.exists(sql_file):
              with open(sql_file, "r", encoding="utf-8") as f:
                  existing = set(line.strip() for line in f if line.strip())

          # --- Ghi các dòng mới ---
          new_lines = []
          for _, row in df.iterrows():
              def v(x):
                  if pd.isna(x) or str(x).lower() in ["none", "nan", "null", ""]:
                      return "NULL"
                  return f"'{str(x).replace(\"'\", \"''\")}'"

              line = f"INSERT INTO phongtro_danang VALUES ({v(row.get('location'))}, {v(row.get('price'))}, {v(row.get('area'))}, {v(row.get('date_posted'))}, {v(row.get('fridge'))}, {v(row.get('washer'))}, {v(row.get('air_condition'))}, {v(row.get('wifi'))});"

              if line not in existing:
                  new_lines.append(line)

          if new_lines:
              with open(sql_file, "a", encoding="utf-8") as f:
                  for line in new_lines:
                      f.write("\n" + line)
              print(f"✅ Đã thêm {len(new_lines)} dòng mới vào SQL.")
          else:
              print("⚠️ Không có dòng mới để thêm.")
          EOF

      # --- 4️⃣ Commit & Push ---
      - name: Commit & Push Changes
        run: |
          git config user.name "github-actions"
          git config user.email "actions@github.com"
          git add phongtro_danang.sql || true
          git commit -m "Daily update from both notebooks (merged unique rows)" || echo "No changes"
          git push || echo "No changes to push"
